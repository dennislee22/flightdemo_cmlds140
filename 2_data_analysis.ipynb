{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Data Analysis\n",
    "\n",
    "For this project, the requirement is to use the flights dataset to predict if a particular flight in the future will be cancelled. This first notebook is used to explore the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#The various imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark_llap.sql.session import HiveWarehouseSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Spark session\n",
    "Connect to spark using the standard Spark Session connector. I've put the connection parameters into each file directly they change dependingo the type of job that is running. You should adjust the following for your specific Spark environment.\n",
    "\n",
    "+ `spark.executor.memory`\n",
    "+ `spark.executor.cores`\n",
    "+ `spark.driver.memory` \n",
    "\n",
    "Spark will use the default `master` setting when connecting to the resource manager. With Cloudera CML, this will be Spark on Kubernetes. If you are working on you local machine for testing, add `.master(\"local[*]\")\\` before `.getOrCreate()`     \n",
    "\n",
    "The next on is specific to the Cloudera CML setup:\n",
    "\n",
    "`spark.yarn.access.hadoopFileSystems s3a://[s3 bucket location]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "storage = os.getenv(\"STORAGE\")\n",
    "storage = \"/tmp\"\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Airline Data Exploration\")\\\n",
    "    .config(\"spark.security.credentials.hiveserver2.enabled\",\"false\")\\\n",
    "    .config(\"spark.datasource.hive.warehouse.read.jdbc.mode\", \"client\")\\\n",
    "    .config(\"spark.jars\", \"/home/cdsw/hive-warehouse-connector-assembly-1.0.0.7.1.7.1000-141.jar\")\\\n",
    "    .config(\"spark.sql.hive.hiveserver2.jdbc.url\",\"jdbc:hive2://hs2-hiveflight.apps.ecs1.cdpkvm.cldr/flight;transportMode=http;httpPath=cliservice;socketTimeout=60;ssl=true;retries=3;user=ldapuser1;password=ldapuser1\")\\\n",
    "    .config(\"spark.executor.memory\",\"8g\")\\\n",
    "    .config(\"spark.executor.cores\",\"2\")\\\n",
    "    .config(\"spark.driver.memory\",\"6g\")\\\n",
    "    .config(\"spark.yarn.access.hadoopFileSystems\",storage)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Spark UI\n",
    "This creates a link the Spark UI. Its specific to CML and needed because of an issue wit TLS that is being fixed. If you are running locally, just run `spark` as a command in a cell and it will provide you with the Spark UI link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://spark-5woymrf6iepl5zl8.ml-541f9e83-006.apps.ecs1.cdpkvm.cldr\" target=\"_blank\">Spark UI</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML('<a href=\"http://spark-{}.{}\" target=\"_blank\">Spark UI</a>'.format(os.getenv(\"CDSW_ENGINE_ID\"),os.getenv(\"CDSW_DOMAIN\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data\n",
    "This file was downloaded from [Kaggle](https://www.kaggle.com/yuanyuwendymu/airline-delay-and-cancellation-data-2009-2018) as a CSV and upload to S3. Since we know the schema already, we can make its correct by defining the schema for the import rather than relying on inferSchema. Its also faster! \n",
    "\n",
    "_Note: If you are working in local mode, you should limit the number of rows that are returned._\n",
    "\n",
    "> _HANDY TIP_\n",
    "> \n",
    "> Use `.persist()` on a data frame that you are work a lot with to prevent Spark from fetching the data everytime you run query. It will store that dataframe in memory and all operations on that dataframe will run on the in-memory version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#flight_df = spark.sql(\"select * from default.flights_data_all\")\n",
    "\n",
    "# Uncomment this if you are running in local mode and just want to see things work\n",
    "#flight_df = flight_df.limit(10000)\n",
    "hive = HiveWarehouseSession.session(spark).build()\n",
    "flight_df = hive.sql(\"select * from flight.flights_data_all\")\n",
    "flight_df.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancelled Flights by Carrier\n",
    "The first bit of data exploration is to check the flight cancellations by carrier. This is best done by showing which carrier has the highet percentage of cancelled flights rather than the total number of cancelled flights. \n",
    "\n",
    "Concepts introduced in this section:\n",
    "+ `filter()`,`groupby` etc.\n",
    "+ `withColumn` and `withColumnRenamed`\n",
    "+ `toPandas()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cancel_by_carrier = flight_df\\\n",
    "  .filter(\"CANCELLED == 1\")\\\n",
    "  .groupby(\"OP_CARRIER\")\\\n",
    "  .count()\\\n",
    "  .sort(\"count\",ascending=False)\\\n",
    "  .withColumnRenamed('count', 'count_delays')\n",
    "\n",
    "  \n",
    "flight_by_carrier = flight_df\\\n",
    "  .groupby(\"OP_CARRIER\")\\\n",
    "  .count()\\\n",
    "  .sort(\"count\",ascending=False)\\\n",
    "  .withColumnRenamed('count', 'count_total')\n",
    "\n",
    "  \n",
    "cancel_by_carrier_percent = flight_by_carrier\\\n",
    "  .join(\n",
    "    cancel_by_carrier, \n",
    "    flight_by_carrier.OP_CARRIER == cancel_by_carrier.OP_CARRIER\n",
    "  )\n",
    "  \n",
    "cancel_by_carrier_percent = cancel_by_carrier_percent\\\n",
    "  .withColumn(\n",
    "    \"delay_percent\",(\n",
    "      cancel_by_carrier_percent.count_delays/cancel_by_carrier_percent.count_total\n",
    "    )*100\n",
    "  )\\\n",
    "  .sort(\"delay_percent\",ascending=False)\\\n",
    "  .toPandas()\n",
    "\n",
    "# This limits\n",
    "cancel_by_carrier_percent.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cancelled flights by Year\n",
    "This is not necessarily useful as a predictive metric, but it is still interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "flight_by_year = flight_df\\\n",
    "  .withColumn('year', year('FL_DATE'))\\\n",
    "  .groupby(\"year\")\\\n",
    "  .count()\\\n",
    "  .sort(\"count\",ascending=False)\\\n",
    "  .withColumnRenamed('count', 'count_total')\n",
    "  \n",
    "cancel_by_year = flight_df\\\n",
    "  .filter(\"cancelled == 1\")\\\n",
    "  .withColumn('year', year('FL_DATE'))\\\n",
    "  .withColumnRenamed(\"year\",\"year_cancel\")\\\n",
    "  .groupby(\"year_cancel\")\\\n",
    "  .count()\\\n",
    "  .sort(\"count\",ascending=False)\\\n",
    "  .withColumnRenamed('count', 'cancel_total')\n",
    "\n",
    "cancel_by_year_percent = flight_by_year\\\n",
    "  .join(\n",
    "    cancel_by_year, \n",
    "    flight_by_year.year == cancel_by_year.year_cancel\n",
    "  )  \n",
    "  \n",
    "cancel_by_year_percent = cancel_by_year_percent\\\n",
    "  .withColumn(\n",
    "    \"delay_percent\",(\n",
    "      cancel_by_year_percent.cancel_total/cancel_by_year_percent.count_total\n",
    "    )*100\n",
    "  )\\\n",
    "  .sort(\"year\",ascending=False)\n",
    "  \n",
    "cancel_by_year_percent_pd = cancel_by_year_percent.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot this using a Tufte-like layout :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"white\",{'axes.axisbelow': False})\n",
    "plt.bar( \n",
    "cancel_by_year_percent_pd.year, \n",
    "cancel_by_year_percent_pd.delay_percent,\n",
    "align='center', \n",
    "alpha=0.5,\n",
    "color='#888888',\n",
    ")\n",
    "plt.grid(color='#FFFFFF', linestyle='-', linewidth=0.5, axis='y')\n",
    "plt.title(\n",
    "'Percentage Cancelled Flights by Year',\n",
    "color='grey'\n",
    ")\n",
    "plt.xticks(\n",
    "cancel_by_year_percent_pd.year,\n",
    "color='grey',\n",
    "rotation=45    \n",
    ")\n",
    "plt.yticks(color='grey')\n",
    "sns.despine(left=True,bottom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancelled flights per Week of Year\n",
    "This is a more intersting statistic and likely to have more predictive power. Week of Year will be a seasonal and generally the flight patterns will have busier vs less busy times of the year. This will also show the effect that seasonal weather conditions can have on flight cancellations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "flight_by_week = flight_df\\\n",
    "  .withColumn(\"week\",weekofyear(\"FL_DATE\"))\\\n",
    "  .groupby(\"week\")\\\n",
    "  .count()\\\n",
    "  .sort(\"count\",ascending=False)\\\n",
    "  .withColumnRenamed('count', 'count_total')\n",
    "  \n",
    "cancel_by_week = flight_df\\\n",
    "  .filter(\"cancelled == 1\")\\\n",
    "  .withColumn(\"week\",weekofyear(\"FL_DATE\"))\\\n",
    "  .withColumnRenamed(\"week\",\"week_cancel\")\\\n",
    "  .groupby(\"week_cancel\")\\\n",
    "  .count()\\\n",
    "  .sort(\"count\",ascending=False)\\\n",
    "  .withColumnRenamed('count', 'cancel_total')\n",
    "\n",
    "cancel_by_week_percent = flight_by_week\\\n",
    "  .join(\n",
    "    cancel_by_week, \n",
    "    flight_by_week.week == cancel_by_week.week_cancel\n",
    "  )  \n",
    "  \n",
    "cancel_by_week_percent = cancel_by_week_percent\\\n",
    "  .withColumn(\n",
    "    \"delay_percent\",(\n",
    "      cancel_by_week_percent.cancel_total/cancel_by_week_percent.count_total\n",
    "    )*100\n",
    "  )\\\n",
    "  .sort(\"week\",ascending=False)\n",
    "  \n",
    "cancel_by_week_percent_pd = cancel_by_week_percent.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plotter():\n",
    "  sns.set_style(\"white\",{'axes.axisbelow': False})\n",
    "  plt.bar( \n",
    "    cancel_by_week_percent_pd.week, \n",
    "    cancel_by_week_percent_pd.delay_percent,\n",
    "    align='center', \n",
    "    alpha=0.5,\n",
    "    color='#888888',\n",
    "  )\n",
    "  plt.grid(color='#FFFFFF', linestyle='-', linewidth=0.5, axis='y')\n",
    "  plt.title(\n",
    "    'Percentage Cancelled Flights by Week',\n",
    "    color='grey'\n",
    "  )\n",
    "  plt.xticks(\n",
    "    color='grey',\n",
    "    rotation=45\n",
    "  )\n",
    "  plt.yticks(color='grey')\n",
    "  sns.despine(left=True,bottom=True)\n",
    "plotter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Cancelled Routes\n",
    "To work out if route is likely to cancalled, the easiest way is to create a string that combines the origin and destination. However that works only in on direction, so calculate for both directions, the code below uses `hash` to create an interger that is the sum of the a of the origin and a hash of the destination. This creates a commutative process for any route.\n",
    "\n",
    "\n",
    "> HANDY TIP\n",
    "> In pyspark you can aggregate inside a `select` function (i.e. use `sum` etc) if its not after a groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_routes = flight_df\\\n",
    "  .withColumn(\"combo_hash\", hash(\"ORIGIN\")+hash(\"DEST\"))\\\n",
    "  .withColumn(\"combo\", concat(col(\"ORIGIN\"),col(\"DEST\")))\\\n",
    "  .groupby(\"combo_hash\")\\\n",
    "  .agg(count(\"combo_hash\").alias(\"count_all\"),first(\"combo\").alias(\"route_alias_all\"))\\\n",
    "  .sort(\"count_all\",ascending=False)\n",
    "\n",
    "cancelled_routes_all = flight_df\\\n",
    "  .filter(\"cancelled == 1\")\\\n",
    "  .withColumn(\"combo_hash\", hash(\"ORIGIN\")+hash(\"DEST\"))\\\n",
    "  .withColumn(\"combo\", concat(col(\"ORIGIN\"),col(\"DEST\")))\\\n",
    "  .groupby(\"combo_hash\")\\\n",
    "  .agg(count(\"combo_hash\").alias(\"count\"),first(\"combo\").alias(\"route_alias\"))\\\n",
    "  .sort(\"count\",ascending=False)  \n",
    "\n",
    "cancelled_routes_percentage = cancelled_routes_all\\\n",
    "  .join(\n",
    "    all_routes,\n",
    "    cancelled_routes_all.combo_hash == all_routes.combo_hash\n",
    "  )\\\n",
    "  .withColumn(\n",
    "    \"route\", \n",
    "    concat(\n",
    "      substring(col(\"route_alias\"),0,3),\n",
    "      lit(\"<>\"),\n",
    "      substring(col(\"route_alias\"),4,6)\n",
    "    )\n",
    "  )\\\n",
    "  .withColumn(\n",
    "    \"cancelled_percent\", \n",
    "    col(\"count\")/col(\"count_all\")*100\n",
    "  )\\\n",
    "  .select(\"route\",\"count\",\"count_all\",\"cancelled_percent\")\\\n",
    "  .sort(\"cancelled_percent\",ascending=False)\n",
    "\n",
    "cancelled_routes_percentage.toPandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side Note\n",
    "Interestingly most popular routes have similar numbers of cancelled flights in either direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancelled_by_route_non_combo = flight_df\\\n",
    "  .filter(\"CANCELLED == 1\")\\\n",
    "  .withColumn(\"combo\", concat(col(\"ORIGIN\"),col(\"DEST\")))\\\n",
    "  .groupby(\"combo\")\\\n",
    "  .count()\\\n",
    "  .sort(\"count\",ascending=False)\n",
    "\n",
    "cancelled_by_route_non_combo.toPandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Unused Columns\n",
    "Given that our aim is to calculate a prediction for the CANCELLED variable, many of the other columns are no longer relevant. You don't have an actual wheels down time for a cancelled flight. The code below lists the colums that have lots of NA values on cancelled flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unused_columns = flight_df\\\n",
    "  .filter(\"CANCELLED == 1\")\\\n",
    "  .agg(*(sum(isnull(c).cast('int')).alias(c) for c in flight_df.columns))\n",
    "\n",
    "unused_columns_df = unused_columns.toPandas()\n",
    "unused_columns_df_t = unused_columns_df.transpose()\n",
    "unused_columns_df_t.rename( columns={0 :'count'}, inplace=True )\n",
    "unused_columns_df_t.query('count>0').index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

